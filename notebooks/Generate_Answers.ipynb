{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Generate_Answers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akePZhluuAeh",
        "outputId": "8a8321c8-d989-492c-e6a1-342999264c8a"
      },
      "source": [
        "!pip install gpt-2-simple"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gpt-2-simple in /usr/local/lib/python3.6/dist-packages (0.7.1)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (1.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (1.19.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vifibKZuo07",
        "outputId": "7878688d-b665-418e-8bab-898721e70938"
      },
      "source": [
        "from datetime import datetime\r\n",
        "from google.colab import files\r\n",
        "%tensorflow_version 1.x\r\n",
        "import gpt_2_simple as gpt2\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAbLSmZnuo9E"
      },
      "source": [
        "# gpt2.download_gpt2(model_name=\"355M\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_2OFIddupE0",
        "outputId": "6475b3fb-efd8-4c90-b237-656ca93e95cd"
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg7SGl2iwin3"
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='model-3.0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0bV83JTwiuK"
      },
      "source": [
        "test_file = \"test_descriptions.csv\"\r\n",
        "gpt2.copy_file_from_gdrive(test_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhTfmNWR8CcL"
      },
      "source": [
        "import argparse\r\n",
        "import re\r\n",
        "ANSWER_PATTERN = re.compile(r'<\\|startofname\\|>(.+?)<\\|endofname\\|>')\r\n",
        "NUMBER_OF_ANSWERS_GENERATED = 64\r\n",
        "NUMBER_OF_ANSWERS_SELECTED = 64\r\n",
        "SUBMISSION_ROW = '{}\\n'\r\n",
        "THREAD_COUNT = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebMvRm7CIPMw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtcYD2CG_4zr"
      },
      "source": [
        "def sanitize_input(input):\r\n",
        "    \"\"\"\r\n",
        "    Removes break line html task and | symbol from input string\r\n",
        "    \"\"\"\r\n",
        "    if input.startswith('\"') and input.endswith('\"'):\r\n",
        "        input = input[1:-1]\r\n",
        "\r\n",
        "    return input \\\r\n",
        "        .replace('<br>', '') \\\r\n",
        "        .replace('</br>', '') \\\r\n",
        "        .replace('<br/>', '') \\\r\n",
        "        .replace('|', '') \\\r\n",
        "        .upper()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qkf6re8upSI"
      },
      "source": [
        "\r\n",
        "def sanitize_answer(item):\r\n",
        "    \"\"\"\r\n",
        "    Cleans up answer given by a model.\r\n",
        "    - removes commas (reserved for item name separation in submission file)\r\n",
        "    - strips ending dots\r\n",
        "    :param item:\r\n",
        "    :return:\r\n",
        "    \"\"\"\r\n",
        "    item = item \\\r\n",
        "        .upper() \\\r\n",
        "        .replace(',', '') \\\r\n",
        "        .strip()\r\n",
        "\r\n",
        "    if item.endswith('.'):\r\n",
        "        return item[0:-1]\r\n",
        "    else:\r\n",
        "        return item"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBM9Zc-K2DTN"
      },
      "source": [
        "def extract_answer(model_output):\r\n",
        "    \"\"\"\r\n",
        "    Extracts answer from model output\r\n",
        "    using ANSWER_PATTERN regular expression\r\n",
        "    :param model_output: single element of model generation result\r\n",
        "    :return: extracted answer\r\n",
        "    \"\"\"\r\n",
        "    matched_answers = re.findall(ANSWER_PATTERN, model_output)\r\n",
        "    if len(matched_answers) > 0:\r\n",
        "        return sanitize_answer(matched_answers[0])\r\n",
        "    else:\r\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjlNXLGm2Dl4"
      },
      "source": [
        "def answer_quality(answer, prompt):\r\n",
        "    \"\"\"\r\n",
        "    Calculates heuristics of answer quality in\r\n",
        "    the context of prompt given to the model\r\n",
        "    :param answer: answer extracted from model output\r\n",
        "    :param prompt: prompt given to the model\r\n",
        "    :return: float value between 0 and 1. 0 - answer considered nonsensical, 1 - answer considered exact\r\n",
        "    \"\"\"\r\n",
        "    return 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnH_iDH12e2f"
      },
      "source": [
        "def without_duplicates(items):\r\n",
        "    \"\"\"\r\n",
        "    Removes duplicates from given list\r\n",
        "    :param items:\r\n",
        "    :return: list of unique items\r\n",
        "    \"\"\"\r\n",
        "    return list(dict.fromkeys(items))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "365h7UfN8iOI"
      },
      "source": [
        "import pandas as pd\r\n",
        "def load_test_descriptions(test_descriptions_file_path):\r\n",
        "    \"\"\"\r\n",
        "    :param test_descriptions_file_path: path to the single column CSV file containing item descriptions\r\n",
        "    :return: list of item descriptions\r\n",
        "    \"\"\"\r\n",
        "    test_descriptions = pd.read_csv(test_descriptions_file_path, sep=\"~\")\r\n",
        "    return test_descriptions['description'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMmPrLYt2fBT"
      },
      "source": [
        "def select_answers(outputs, prompt):\r\n",
        "    \"\"\"\r\n",
        "    Returns top answers according to extract_answer heuristics\r\n",
        "    :param outputs: output generated by the model\r\n",
        "    :param prompt: prompt give to the model\r\n",
        "    :return: list of top NUMBER_OF_ANSWERS SELECTED answers sorted according to extarct_answer quality heuristics\r\n",
        "    \"\"\"\r\n",
        "    answers = sorted(\r\n",
        "        without_duplicates(\r\n",
        "            list(\r\n",
        "                filter(\r\n",
        "                    lambda x: x is not None,\r\n",
        "                    [extract_answer(output) for output in outputs if output is not None]\r\n",
        "                )\r\n",
        "            )\r\n",
        "        ),\r\n",
        "        key=lambda x: answer_quality(x, prompt),\r\n",
        "        reverse=True\r\n",
        "    )[: NUMBER_OF_ANSWERS_SELECTED]\r\n",
        "    return answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r8NaAky-ZQY"
      },
      "source": [
        "def generate_model_outputs(input):\r\n",
        "    \"\"\"\r\n",
        "    Generates NUMBER_OF_ANSWERS_GENERATED answers\r\n",
        "    using gpt-2 model loaded in TF Session from\r\n",
        "    given input\r\n",
        "    :param input: gpt-2 prompt (starting text)\r\n",
        "    :return: list of model results\r\n",
        "    \"\"\"\r\n",
        "    gpt_2_prompt = \"<|startoftext|> <|startofdesc|> {} <|endofdesc|> <|startofname|>\"\r\n",
        "    description = gpt_2_prompt.format(sanitize_input(input))\r\n",
        "    return gpt2.generate(\r\n",
        "        tf_sess,\r\n",
        "        temperature=1.0,\r\n",
        "        length=60,\r\n",
        "        nsamples=NUMBER_OF_ANSWERS_GENERATED,\r\n",
        "        prefix=description,\r\n",
        "        run_name='model-3.0',\r\n",
        "        return_as_list=True,\r\n",
        "        seed=666\r\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMuXpCry2fMQ"
      },
      "source": [
        "def generate_item_name_candidates(description, step, on_finished):\r\n",
        "    \"\"\"\r\n",
        "    Generates list of item name candidates sorted according to\r\n",
        "    quality heuristics generated from given item description\r\n",
        "    :param step: number of a step to be executed\r\n",
        "    :param description: single description from test_descriptions.csv file\r\n",
        "    :param on_finished: callback on item generated\r\n",
        "    :return:\r\n",
        "    \"\"\"\r\n",
        "    reset_model(step)\r\n",
        "    outputs = generate_model_outputs(description)\r\n",
        "    answers = select_answers(outputs, description)\r\n",
        "    on_finished()\r\n",
        "    return ', '.join(answers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcrISpfT81lP",
        "outputId": "1bd7e383-924c-4c5f-cb85-7ea3dd279d33"
      },
      "source": [
        "!pip install alive_progress"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: alive_progress in /usr/local/lib/python3.6/dist-packages (1.6.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r690P9ci8Jsy"
      },
      "source": [
        "import alive_progress as ap\r\n",
        "def generate_answers_file(test_descriptions_file_path, answer_file_path):\r\n",
        "    \"\"\"\r\n",
        "    Writes names generated by model to a file\r\n",
        "    \"\"\"\r\n",
        "    descriptions = load_test_descriptions(test_descriptions_file_path)\r\n",
        "    with ap.alive_bar(len(descriptions), bar='filling') as bar:\r\n",
        "        names = [generate_item_name_candidates(description, index, bar)\r\n",
        "                 for (index, description)\r\n",
        "                 in enumerate(descriptions)]\r\n",
        "\r\n",
        "    with open(answer_file_path, 'w') as file:\r\n",
        "        file.write(SUBMISSION_ROW.format('name'))\r\n",
        "        for item_name_candidates in names:\r\n",
        "            file.write(SUBMISSION_ROW.format(item_name_candidates))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QicaYpa7IYb3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCdoaDpT2Dyf",
        "outputId": "6d1763c9-fcb9-439b-e24b-c1c27f61012f"
      },
      "source": [
        "tf_sess = gpt2.start_tf_sess(threads=4)\r\n",
        "gpt2.load_gpt2(tf_sess, run_name='model-3.0', multi_gpu=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/model-3.0/model-403\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/model-3.0/model-403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aumks03bIZKH"
      },
      "source": [
        "def load_model():\r\n",
        "    gpt2.load_gpt2(\r\n",
        "        tf_sess,\r\n",
        "        run_name='model-3.0',\r\n",
        "        multi_gpu=True\r\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBX0zhV_InKU"
      },
      "source": [
        "def reset_model(step_count):\r\n",
        "    global tf_sess\r\n",
        "    if step_count > 0 and step_count % 10 == 0:\r\n",
        "        tf_session = gpt2.reset_session(sess=tf_sess)\r\n",
        "        load_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "3Ra67zYS2D6w",
        "outputId": "ae84efcb-65dc-4d3b-931a-02a6038ba9d9"
      },
      "source": [
        "generate_answers_file(test_file, 'content/drive/submission.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-6f66e53bb690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_answers_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content/drive/submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-84d421742fa8>\u001b[0m in \u001b[0;36mgenerate_answers_file\u001b[0;34m(test_descriptions_file_path, answer_file_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m         names = [generate_item_name_candidates(description, index, bar)\n\u001b[1;32m      9\u001b[0m                  \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                  in enumerate(descriptions)]\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-84d421742fa8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malive_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'filling'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         names = [generate_item_name_candidates(description, index, bar)\n\u001b[0;32m----> 9\u001b[0;31m                  \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                  in enumerate(descriptions)]\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-158c4e5e0c67>\u001b[0m in \u001b[0;36mgenerate_item_name_candidates\u001b[0;34m(description, step, on_finished)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mreset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_model_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_answers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-4ea8758750c1>\u001b[0m in \u001b[0;36mreset_model\u001b[0;34m(step_count)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep_count\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtf_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_sess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-cdb50c86b942>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtf_sess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model-3.0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mmulti_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mload_gpt2\u001b[0;34m(sess, run_name, checkpoint_dir, model_name, model_dir, multi_gpu)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallow_empty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luWxuYeyupX1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptI-I5yAupci"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}